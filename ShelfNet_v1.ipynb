{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ShelfNet_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14XVNCIwIuZ6k6YlC8UCrxLo1j9_HRErB",
      "authorship_tag": "ABX9TyP9kjFoccnj80zZMWvCvsUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varghese-Kuruvilla/Road-Segmentation/blob/master/ShelfNet_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5N5bdoxud5O"
      },
      "source": [
        "#Startup code on google colab\n",
        "# !unzip /content/drive/My\\ Drive/Rbccps/Semantic-segmentation/ShelfNet_v1/ShelfNet18_realtime.zip -d /content/\n",
        "# !unzip /content/drive/My\\ Drive/Rbccps/Semantic-segmentation/ShelfNet_v1/homography_computation.zip -d /content/\n",
        "#!pip install ninja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjGvMIA-yThW"
      },
      "source": [
        "#Command to zip and save to google drive\n",
        "# !zip -r ShelfNet18_realtime.zip ShelfNet18_realtime/\n",
        "# !zip -r homography_computation.zip homography_computation\n",
        "# mv ShelfNet18_realtime.zip /content/drive/My\\ Drive/Rbccps/Semantic-segmentation/ShelfNet_v1\n",
        "# mv homography_computation.zip /content/drive/My\\ Drive/Rbccps/Semantic-segmentation/ShelfNet_v1/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr6ORx2KXOPx",
        "outputId": "ae8f46e8-efad-433c-9071-75695522ee8c"
      },
      "source": [
        "cd ShelfNet18_realtime/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ShelfNet18_realtime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ddjUjQcXk2YK",
        "outputId": "55561354-aa1a-47f1-cf1e-5be6e691d753"
      },
      "source": [
        "%%writefile /content/ShelfNet18_realtime/evaluate.py\n",
        "#!/usr/bin/python\n",
        "# -*- encoding: utf-8 -*-\n",
        "from logger import setup_logger\n",
        "from cityscapes import CityScapes\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import logging\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from PIL import Image\n",
        "from shelfnet import ShelfNet\n",
        "\n",
        "\n",
        "class MscEval(object):\n",
        "    def __init__(self,\n",
        "            model,\n",
        "            dataloader,\n",
        "            scales = [ 1.0],\n",
        "            n_classes = 19,#19\n",
        "            lb_ignore = 255,\n",
        "            cropsize = 1024,\n",
        "            flip = False,\n",
        "            *args, **kwargs):\n",
        "        self.scales = scales\n",
        "        self.n_classes = n_classes\n",
        "        self.lb_ignore = lb_ignore\n",
        "        self.flip = flip\n",
        "        self.cropsize = cropsize\n",
        "        ## dataloader\n",
        "        self.dl = dataloader\n",
        "        self.net = model\n",
        "\n",
        "\n",
        "    def pad_tensor(self, inten, size):\n",
        "        N, C, H, W = inten.size()\n",
        "        outten = torch.zeros(N, C, size[0], size[1]).cuda()\n",
        "        outten.requires_grad = False\n",
        "        margin_h, margin_w = size[0]-H, size[1]-W\n",
        "        hst, hed = margin_h//2, margin_h//2+H\n",
        "        wst, wed = margin_w//2, margin_w//2+W\n",
        "        outten[:, :, hst:hed, wst:wed] = inten\n",
        "        return outten, [hst, hed, wst, wed]\n",
        "\n",
        "\n",
        "    def eval_chip(self, crop):\n",
        "        with torch.no_grad():\n",
        "            out = self.net(crop)[0]\n",
        "            prob = F.softmax(out, 1)\n",
        "            if self.flip:\n",
        "                crop = torch.flip(crop, dims=(3,))\n",
        "                out = self.net(crop)[0]\n",
        "                out = torch.flip(out, dims=(3,))\n",
        "                prob += F.softmax(out, 1)\n",
        "            #prob = torch.exp(prob)\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def crop_eval(self, im):\n",
        "        cropsize = self.cropsize\n",
        "        stride_rate = 1.0\n",
        "        N, C, H, W = im.size()\n",
        "        long_size, short_size = (H,W) if H>W else (W,H)\n",
        "        if long_size < cropsize:\n",
        "            im, indices = self.pad_tensor(im, (cropsize, cropsize))\n",
        "            prob = self.eval_chip(im)\n",
        "            prob = prob[:, :, indices[0]:indices[1], indices[2]:indices[3]]\n",
        "        else:\n",
        "            stride = math.ceil(cropsize*stride_rate)\n",
        "            if short_size < cropsize:\n",
        "                if H < W:\n",
        "                    im, indices = self.pad_tensor(im, (cropsize, W))\n",
        "                else:\n",
        "                    im, indices = self.pad_tensor(im, (H, cropsize))\n",
        "            N, C, H, W = im.size()\n",
        "            n_x = math.ceil((W-cropsize)/stride)+1\n",
        "            n_y = math.ceil((H-cropsize)/stride)+1\n",
        "            prob = torch.zeros(N, self.n_classes, H, W).cuda()\n",
        "            prob.requires_grad = False\n",
        "            for iy in range(n_y):\n",
        "                for ix in range(n_x):\n",
        "                    hed, wed = min(H, stride*iy+cropsize), min(W, stride*ix+cropsize)\n",
        "                    hst, wst = hed-cropsize, wed-cropsize\n",
        "                    chip = im[:, :, hst:hed, wst:wed]\n",
        "                    prob_chip = self.eval_chip(chip)\n",
        "                    prob[:, :, hst:hed, wst:wed] += prob_chip\n",
        "            if short_size < cropsize:\n",
        "                prob = prob[:, :, indices[0]:indices[1], indices[2]:indices[3]]\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def scale_crop_eval(self, im, scale):\n",
        "        N, C, H, W = im.size()\n",
        "        new_hw = [int(H*scale), int(W*scale)]\n",
        "        im = F.interpolate(im, new_hw, mode='bilinear', align_corners=True)\n",
        "        prob = self.crop_eval(im)\n",
        "        prob = F.interpolate(prob, (H, W), mode='bilinear', align_corners=True)\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def compute_hist(self, pred, lb):\n",
        "        n_classes = self.n_classes\n",
        "        ignore_idx = self.lb_ignore\n",
        "        keep = np.logical_not(lb==ignore_idx)\n",
        "        merge = pred[keep] * n_classes + lb[keep]\n",
        "        hist = np.bincount(merge, minlength=n_classes**2)\n",
        "        hist = hist.reshape((n_classes, n_classes))\n",
        "        return hist\n",
        "\n",
        "\n",
        "    def evaluate(self):\n",
        "        ## evaluate\n",
        "        n_classes = self.n_classes\n",
        "        hist = np.zeros((n_classes, n_classes), dtype=np.float32)\n",
        "        dloader = tqdm(self.dl)\n",
        "        if dist.is_initialized() and not dist.get_rank()==0:\n",
        "            dloader = self.dl\n",
        "        for i, (imgs, label) in enumerate(dloader):\n",
        "            # print(\"lkdhsdx\",label.shape)\n",
        "            print(\"imgs.size()\",imgs.size())\n",
        "            print(\"imgs.type()\",imgs.type())\n",
        "            N,_, H, W = label.shape\n",
        "            probs = torch.zeros((N, self.n_classes, H, W))\n",
        "            probs.requires_grad = False\n",
        "            imgs = imgs.cuda()\n",
        "            for sc in self.scales:\n",
        "                prob = self.scale_crop_eval(imgs, sc)\n",
        "                probs += prob.detach().cpu()\n",
        "            probs = probs.data.numpy()\n",
        "            preds = np.argmax(probs, axis=1)\n",
        "\n",
        "            #Changed \n",
        "            # preds_test = preds.squeeze()\n",
        "            # preds_test[preds_test == 7] = 255\n",
        "            # preds_test = preds_test.astype(np.uint8)\n",
        "            # cv2.imwrite(\"inf_\"+str(i) + \".png\",preds_test)           \n",
        "            \n",
        "            #Indicate road region in white\n",
        "            # print(preds,\"dfjdhgduijfduifd\",label.data.numpy().squeeze(1))\n",
        "            # palette = np.random.randint(0, 256, (256, 3), dtype=np.uint8)\n",
        "            # pred = palette[preds[1]]\n",
        "            # cv2.imwrite(\"./res/{}.png\".format(i),pred)\n",
        "            # print(\"sllkjdisydhkasnalk\",preds[1])\n",
        "            \n",
        "#             out_img = Image.fromarray(preds[1])\n",
        "#             citypall=[\n",
        "# 128,64,128,244,35,232,70,70,70,102,102,156,190,153,153,153,153,153,250,170,30,220,220,0,107,142,35,152,251,152,70,130,180,220,20,60,255,0,0,0,0,142,0,0,70,0,60,100,0,80,100,0,0,230,119,11,32,128,192,0,0,64,128,128,64,128,0,192,128,128,192,128,64,64,0,192,64,0,64,192,0,192,192,0,64,64,128,192,64,128,64,192,128,192,192,128,0,0,64,128,0,64,0,128,64,128,128,64,0,0,192,128,0,192,0,128,192,128,128,192,64,0,64,192,0,64,64,128,64,192,128,64,64,0,192,192,0,192,64,128,192,192,128,192,0,64,64,128,64,64,0,192,64,128,192,64,0,64,192,128,64,192,0,192,192,128,192,192,64,64,64,192,64,64,64,192,64,192,192,64,64,64,192,192,64,192,64,192,192,192,192,192,32,0,0,160,0,0,32,128,0,160,128,0,32,0,128,160,0,128,32,128,128,160,128,128,96,0,0,224,0,0,96,128,0,224,128,0,96,0,128,224,0,128,96,128,128,224,128,128,32,64,0,160,64,0,32,192,0,160,192,0,32,64,128,160,64,128,32,192,128,160,192,128,96,64,0,224,64,0,96,192,0,224,192,0,96,64,128,224,64,128,96,192,128,224,192,128,32,0,64,160,0,64,32,128,64,160,128,64,32,0,192,160,0,192,32,128,192,160,128,192,96,0,64,224,0,64,96,128,64,224,128,64,96,0,192,224,0,192,96,128,192,224,128,192,32,64,64,160,64,64,32,192,64,160,192,64,32,64,192,160,64,192,32,192,192,160,192,192,96,64,64,224,64,64,96,192,64,224,192,64,96,64,192,224,64,192,96,192,192,224,192,192,0,32,0,128,32,0,0,160,0,128,160,0,0,32,128,128,32,128,0,160,128,128,160,128,64,32,0,192,32,0,64,160,0,192,160,0,64,32,128,192,32,128,64,160,128,192,160,128,0,96,0,128,96,0,0,224,0,128,224,0,0,96,128,128,96,128,0,224,128,128,224,128,64,96,0,192,96,0,64,224,0,192,224,0,64,96,128,192,96,128,64,224,128,192,224,128,0,32,64,128,32,64,0,160,64,128,160,64,0,32,192,128,32,192,0,160,192,128,160,192,64,32,64,192,32,64,64,160,64,192,160,64,64,32,192,192,32,192,64,160,192,192,160,192,0,96,64,128,96,64,0,224,64,128,224,64,0,96,192,128,96,192,0,224,192,128,224,192,64,96,64,192,96,64,64,224,64,192,224,64,64,96,192,192,96,192,64,224,192,192,224,192,32,32,0,160,32,0,32,160,0,160,160,0,32,32,128,160,32,128,32,160,128,160,160,128,96,32,0,224,32,0,96,160,0,224,160,0,96,32,128,224,32,128,96,160,128,224,160,128,32,96,0,160,96,0,32,224,0,160,224,0,32,96,128,160,96,128,32,224,128,160,224,128,96,96,0,224,96,0,96,224,0,224,224,0,96,96,128,224,96,128,96,224,128,224,224,128,32,32,64,160,32,64,32,160,64,160,160,64,32,32,192,160,32,192,32,160,192,160,160,192,96,32,64,224,32,64,96,160,64,224,160,64,96,32,192,224,32,192,96,160,192,224,160,192,32,96,64,160,96,64,32,224,64,160,224,64,32,96,192,160,96,192,32,224,192,160,224,192,96,96,64,224,96,64,96,224,64,224,224,64,96,96,192,224,96,192,96,224,192,0,0,0]\n",
        "#             outimg=out_img.putpalette(citypall)\n",
        "#             print(\"kshdshd\",outimg)\n",
        "            \n",
        "\n",
        "            # hist_once = self.compute_hist(preds, label.data.numpy().squeeze(1))\n",
        "            # print(hist_once.shape)\n",
        "            # hist = hist + hist_once\n",
        "        # print(\"jhusgds\",hist)\n",
        "        # print(\"kdkdfkdmf\",np.sum(hist, axis=0),\"dfdfkdfjdkfd\",np.sum(hist, axis=1),\"dnfdjfdnfdjf\",np.diag(hist))\n",
        "        # print(\"diagggagg\",np.diag(hist))\n",
        "        # print(\"axisssisis\",(np.sum(hist, axis=0)+np.sum(hist, axis=1)-np.diag(hist)))\n",
        "        #Changed\n",
        "        # palette = np.random.randint(0, 256, (256, 3), dtype=np.uint8)\n",
        "        # try:\n",
        "        #     pred = palette[preds[1]]\n",
        "        #     pred = palette[preds]\n",
        "        #     cv2.imwrite(\"./result.png\",pred)\n",
        "        # except Exception as e:\n",
        "        #     print(e)\n",
        "\n",
        "        # IOUs = np.diag(hist) / (np.sum(hist, axis=0)+np.sum(hist, axis=1)-np.diag(hist))\n",
        "        # print(\"iouououo\",IOUs)\n",
        "        # mIOU = np.mean(IOUs)\n",
        "        # return mIOU\n",
        "        return 0\n",
        "\n",
        "\n",
        "def evaluate(respth='./res', dspth='./data/cityscapes', checkpoint=None):\n",
        "    ## logger\n",
        "    logger = logging.getLogger()\n",
        "\n",
        "    ## model\n",
        "    logger.info('\\n')\n",
        "    logger.info('===='*20)\n",
        "    logger.info('evaluating the model ...\\n')\n",
        "    logger.info('setup and restore model')\n",
        "    n_classes = 19#19\n",
        "    net = ShelfNet(n_classes=n_classes)\n",
        "\n",
        "    if checkpoint is None:\n",
        "        save_pth = osp.join(respth, 'model_final_iiscidd.pth')\n",
        "    else:\n",
        "        save_pth = checkpoint\n",
        "\n",
        "    net.load_state_dict(torch.load(save_pth))\n",
        "    net.cuda()\n",
        "    net.eval()\n",
        "\n",
        "    ## dataset\n",
        "    batchsize = 1\n",
        "    n_workers = 10\n",
        "    dsval = CityScapes(dspth, mode='test')\n",
        "    # print(\"sjdusgdsds\",dsval)\n",
        "    dl = DataLoader(dsval,\n",
        "                    batch_size = batchsize,\n",
        "                    shuffle = False,\n",
        "                    num_workers = n_workers,\n",
        "                    drop_last = False)\n",
        "    print(\"len(dl)\",len(dl))\n",
        "\n",
        "    ## evaluator\n",
        "    logger.info('compute the mIOU')\n",
        "    evaluator = MscEval(net, dl, scales=[1.0],flip=False)\n",
        "    ## eval\n",
        "    mIOU = evaluator.evaluate()\n",
        "    logger.info('mIOU is: {:.6f}'.format(mIOU))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_logger('./res')    evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/ShelfNet18_realtime/evaluate.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvHVKcYIwQAl"
      },
      "source": [
        "# %%writefile auto_park_utils.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "import numpy as np \n",
        "import glob\n",
        "\n",
        "from evaluate import MscEval\n",
        "from shelfnet import ShelfNet\n",
        "\n",
        "class auto_park_vision():\n",
        "    def __init__(self,weights_path):\n",
        "\n",
        "        self.n_classes = 19\n",
        "        #Homography matrix\n",
        "        self.H = np.array([[1.132372123443505,-3.359375305984110, 843.835898580380217],\n",
        "                [0.604347080571571 ,-0.324814498163494 ,955.938833468783173],\n",
        "                [0.001378608429514 ,-0.000531862797747, 1.000000000000000]])\n",
        "        #Point indicating potential parking spot\n",
        "        self.midpoint = []\n",
        "        self.point_3d = None #3D point corresponding to self.midpoint\n",
        "        self.eval_define() #Define Object of class MscEval\n",
        "        #self.evaluator is an object of the class MscEval\n",
        "\n",
        "    def forward_pass(self,frame=None,img_path=None):\n",
        "\n",
        "        if(img_path != None):\n",
        "            img = Image.open(img_path)\n",
        "        else:\n",
        "            img = frame \n",
        "        orig_img = np.array(img)\n",
        "        # orig_img = cv2.imread(img_path)\n",
        "        # cv2_imshow(img)\n",
        "        #Preprocess Image\n",
        "        to_tensor = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "            ])\n",
        "        img = to_tensor(img)\n",
        "        \n",
        "\n",
        "        _, H, W = img.shape\n",
        "        # print(\"H,W:\",H,W)\n",
        "        #Change image size to the form NCHW from CHW\n",
        "        img = img.unsqueeze(0)\n",
        "\n",
        "        probs = torch.zeros((1, self.n_classes, H, W))\n",
        "        probs.requires_grad = False\n",
        "        img = img.cuda()        \n",
        "\n",
        "        # for sc in self.scales:\n",
        "        prob = self.evaluator.scale_crop_eval(img, scale=1.0) #prob.type torch.cuda.FloatTensor\n",
        "        prob = prob.detach().cpu()\n",
        "        prob = prob.data.numpy()\n",
        "        preds = np.argmax(prob, axis=1) #preds.dtype int64\n",
        "        # palette = np.random.randint(0, 256, (256, 3), dtype=np.uint8)\n",
        "        # pred = palette[preds.squeeze()]\n",
        "\n",
        "        #Changed \n",
        "        preds = preds.squeeze().astype(np.uint8)\n",
        "        preds[preds == 0] = 255\n",
        "        preds = preds.astype(np.uint8)\n",
        "        return preds\n",
        "        # overlay = np.copy(preds)\n",
        "        # overlay = cv2.cvtColor(overlay,cv2.COLOR_GRAY2RGB)\n",
        "        # cv2_imshow(preds)\n",
        "        #Overlay preds over the original image\n",
        "        # alpha = 0.5\n",
        "        # cv2.addWeighted(overlay, alpha, orig_img, 1 - alpha,0, orig_img)\n",
        "        # cv2_imshow(orig_img)\n",
        "        # orig_img = self.parking_spot_detection(preds,orig_img)\n",
        "        # return orig_img\n",
        "\n",
        "    def image_to_world(self,point_2d):\n",
        "        '''\n",
        "        Function to find out world coordinates\n",
        "        given an image point\n",
        "        '''\n",
        "        #TODO: Check homography matrix\n",
        "        # print(\"point.shape\",point.shape)\n",
        "        #Conversion to homogenous coordinates\n",
        "        point_2d = np.append(point_2d,1).reshape(-1,1)\n",
        "\n",
        "        H_inv = np.linalg.inv(self.H)\n",
        "        self.point_3d = np.dot(H_inv,point_2d)\n",
        "        self.point_3d = self.point_3d / self.point_3d[2,0]\n",
        "        return self.point_3d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def eval_define(self):\n",
        "\n",
        "        n_classes = self.n_classes\n",
        "        net = ShelfNet(n_classes=n_classes)\n",
        "\n",
        "        net.load_state_dict(torch.load(weights_path))\n",
        "        net.cuda()\n",
        "        net.eval()\n",
        "        self.evaluator = MscEval(net, dataloader=None, scales=[1.0],flip=False)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     weights_path = '/content/ShelfNet18_realtime/res/model_final_final_iisc_idd_16kweights.pth'\n",
        "#     auto_park_obj = auto_park_vision(weights_path)\n",
        "\n",
        "#     save_num = 0\n",
        "#     count = 0\n",
        "#     servoing_flag = 0\n",
        "\n",
        "#     # cap = cv2.VideoCapture('/content/ShelfNet18_realtime/auto_park_module/video/ece_loop.mp4')\n",
        "#     # while(cap.isOpened()):\n",
        "#         # ret, frame = cap.read()\n",
        "#         # if(ret == True):\n",
        "#         #Convert opencv Image to PIL Image\n",
        "#     for img_path in glob.glob('/content/homography_computation/data/*.png'):\n",
        "#         if(servoing_flag == 0):\n",
        "#             frame = cv2.imread(img_path)\n",
        "#             # frame = cv2.imread('/content/homography_computation/data/rbc_data_00061.png')\n",
        "#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "#             frame_pil = Image.fromarray(frame)\n",
        "#             seg_img = auto_park_obj.forward_pass(frame_pil,img_path=None)\n",
        "#             cv2.imwrite(img_path.split('.png')[0] + '_overlay.png',seg_img)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "iLKGwDvNLfzl",
        "outputId": "21a525f2-01da-49e2-bf49-3102d10819e0"
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import time\n",
        "from statistics import mean\n",
        "#Computed Homography\n",
        "H = np.array([[23.051566426191190, 38.859138964854033, 626.971467292441275],\n",
        "              [5.111361028987754, -1.474310992555276, 7144.855081929711559],\n",
        "              [0.036814327356280, -0.001812769201662, 1.0]])\n",
        "\n",
        "def world_2d(points_ls):\n",
        "    '''\n",
        "    Convert a list of 3d points to corresponding image points using homography\n",
        "    '''\n",
        "    points_2d_ls = []\n",
        "    for point in points_ls:\n",
        "        point_3d = np.asarray(point)\n",
        "        point_3d = point_3d.reshape(3,1)\n",
        "        # print(\"point_3d\",point_3d)\n",
        "        point_2d = np.dot(H,point_3d)\n",
        "        point_2d = point_2d // point_2d[2,0]\n",
        "        points_2d_ls.append(point_2d)\n",
        "        # print(\"point_2d\",point_2d)\n",
        "    return points_2d_ls\n",
        "\n",
        "def project_points(img,points_2d_ls):\n",
        "    '''\n",
        "    Draw a bounding rectangle on the image using the computed homography\n",
        "    '''\n",
        "    # cv2_imshow(img)\n",
        "    # for point_2d in points_2d_ls:\n",
        "    #     cv2.circle(img,(point_2d[0],point_2d[1]), 5, (0,0,255), -1)\n",
        "        \n",
        "    #For visualization\n",
        "    N = len(points_2d_ls)\n",
        "    for i in range(0,N):\n",
        "        cv2.line(img,(points_2d_ls[i%N][0],points_2d_ls[i%N][1]),\\\n",
        "                 (points_2d_ls[(i+1)%N][0],points_2d_ls[(i+1)%N][1]),\\\n",
        "                 (0,0,255),4)\n",
        "    #Image with points drawn on it\n",
        "    cv2_imshow(img)\n",
        "\n",
        "def pot_parking_spot(orig_img,inf_img,points_2d_ls):\n",
        "    '''\n",
        "    Function to detect potential parking spot\n",
        "    '''\n",
        "    #TODO: Optimize the code\n",
        "    xcoords_ls = []\n",
        "    ycoords_ls = []\n",
        "    for point in points_2d_ls:\n",
        "        xcoords_ls.append(point[0][0])\n",
        "        ycoords_ls.append(point[1][0])\n",
        "    \n",
        "    #Line equations of the top and bottom line\n",
        "    coeff_bottom = np.polyfit(xcoords_ls[0:2],ycoords_ls[0:2],1)\n",
        "    line_bottom = np.poly1d(coeff_bottom)\n",
        "\n",
        "    coeff_top = np.polyfit(xcoords_ls[2:4],ycoords_ls[2:4],1)\n",
        "    line_top = np.poly1d(coeff_top)\n",
        "    # print(\"line_bottom\",line_bottom)\n",
        "    # print(\"line_top\",line_top)\n",
        "    flag_top = 0\n",
        "    flag_bottom = 0\n",
        "    #Points for potential parking spot\n",
        "    pt_tl = []\n",
        "    pt_tr = []\n",
        "    pt_bl = []\n",
        "    pt_br = []\n",
        "    for x in range(0,inf_img.shape[1]):\n",
        "        if(inf_img[int(line_top(x)),x] == 255 and flag_top == 0):\n",
        "            pt_tl = [int(line_top(x)),x]\n",
        "            pt_tr = [int(line_top(x+200)),int(x+200)]\n",
        "            # cv2.circle(orig_img,(pt_tl[1],pt_tl[0]), 5, (0,0,255), -1)\n",
        "            # cv2.circle(orig_img,(pt_tr[1],pt_tr[0]), 5, (0,0,255), -1)\n",
        "            # cv2_imshow(orig_img)\n",
        "            flag_top = 1\n",
        "\n",
        "        if(inf_img[int(line_bottom(x)),x] == 255 and flag_bottom == 0):\n",
        "            pt_bl = [int(line_bottom(x)),x]\n",
        "            pt_br = [int(line_bottom(x+200)),int(x+200)]\n",
        "            # cv2.circle(orig_img,(pt_bl[1],pt_bl[0]), 5, (0,0,255), -1)\n",
        "            # cv2.circle(orig_img,(pt_br[1],pt_br[0]), 5, (0,0,255), -1)\n",
        "            # cv2_imshow(orig_img)\n",
        "            flag_bottom = 1\n",
        "        \n",
        "        if(flag_top == 1 and flag_bottom ==1):\n",
        "            # cv2_imshow(orig_img)\n",
        "            break\n",
        "\n",
        "    if(flag_top == 1 and flag_bottom == 1):\n",
        "        cv2.line(orig_img,(pt_tl[1],pt_tl[0]),(pt_tr[1],pt_tr[0]),(0,0,255),2)\n",
        "        cv2.line(orig_img,(pt_tr[1],pt_tr[0]),(pt_br[1],pt_br[0]),(0,0,255),2)\n",
        "        cv2.line(orig_img,(pt_br[1],pt_br[0]),(pt_bl[1],pt_bl[0]),(0,0,255),2)\n",
        "        cv2.line(orig_img,(pt_bl[1],pt_bl[0]),(pt_tl[1],pt_tl[0]),(0,0,255),2)\n",
        "\n",
        "    return orig_img\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # img = cv2.imread('/content/homography_computation/data/rbc_data_00061.png')\n",
        "    weights_path = '/content/ShelfNet18_realtime/res/model_final_final_iisc_idd_16kweights.pth'\n",
        "    auto_park_obj = auto_park_vision(weights_path)\n",
        "    points_ls = [[620,200,1],[620,-200,1],[1120,-200,1],[1120,200,1]]\n",
        "    points_2d_ls = world_2d(points_ls) #[967,427],[295,438],[438,309],[817,300]\n",
        "    # print(\"points_2d_ls\",points_2d_ls)\n",
        "    count = 0 #For debug\n",
        "    avg_time_ls = []\n",
        "    cap = cv2.VideoCapture('/content/homography_computation/data/logi2.webm')\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    size = (frame_width,frame_height)\n",
        "    # for file_path in glob.glob('/content/homography_computation/data/rbc_data_*.png'):\n",
        "    ret = True\n",
        "    result = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc(*'MJPG'),\\\n",
        "                             10,size)\n",
        "    while(ret == True):\n",
        "        ret, frame = cap.read()\n",
        "        # frame = cv2.imread(file_path)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame_pil = Image.fromarray(frame)\n",
        "\n",
        "        seg_img = auto_park_obj.forward_pass(frame_pil,img_path=None)\n",
        "        # print(\"seg_img.shape\",seg_img.shape)\n",
        "        res_img = pot_parking_spot(frame,seg_img,points_2d_ls)\n",
        "        result.write(res_img)\n",
        "        # cv2_imshow(res_img)\n",
        "        # img_name = os.path.basename(inf_path)\n",
        "        # cv2.imwrite(\"/content/homography_computation/parking_spot_results/\"+\\\n",
        "                    # img_name.split('_inf.png')[0] + '_spot.png',res_img)\n",
        "        # count += 1\n",
        "    # project_points(img,points_2d_ls)\n",
        "    result.release()\n",
        "    cap.release()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-208819f7056b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# frame = cv2.imread(file_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mframe_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7iXBrpBY-XU",
        "outputId": "47f6b2e4-3963-4ce3-b6bd-2b6b5f7fc7ec"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ShelfNet18_realtime\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}